{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "In this notebook, we show how to finetune an existing network for a new task. As kind of recap we start with the original model and then create a new model which we finetune that to the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from imagenet_classes import class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original model\n",
    "\n",
    "We start with the original model. There seems to be a competition how to define a network with the least possible amount of code. The slim library includes a **arg_scope** which allows define defaults for aguments and allows to repeat building blocks. With this library we can defined the VGG16 network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "images = tf.placeholder(tf.float32, [None, None, None, 3], name='images')\n",
    "inputs = tf.image.resize_images(images, (224,224))\n",
    "with tf.variable_scope('vgg_16', [inputs]) as sc:\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                          activation_fn=tf.nn.relu,\n",
    "                          weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                          weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "        net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "        net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None,scope='fc8')\n",
    "\n",
    "tf.train.SummaryWriter('/tmp/dumm/fine_tuning', tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables_to_restore = slim.get_variables_to_restore()\n",
    "#init_assign_op, init_feed_dict = \\\n",
    "#   slim.assign_from_checkpoint('/Users/oli/Dropbox/server_sync/tf_slim_models/vgg_16.ckpt', variables_to_restore)\n",
    "init_assign_op, init_feed_dict = \\\n",
    "   slim.assign_from_checkpoint('/home/dueo/Dropbox/Server_Sync//tf_slim_models/vgg_16.ckpt', variables_to_restore)\n",
    "sess = tf.Session()\n",
    "sess.run(init_assign_op, init_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 standard poodle 0.991464\n",
      "265 toy poodle 0.00368469\n",
      "266 miniature poodle 0.0030329\n",
      "160 Afghan hound, Afghan 0.000330886\n",
      "221 Irish water spaniel 0.000312831\n"
     ]
    }
   ],
   "source": [
    "from imagenet_classes import class_names\n",
    "img1 = imread('poodle.jpg')\n",
    "feed_vals = [img1]\n",
    "np.shape(feed_vals)\n",
    "d = sess.run(net, feed_dict={images:feed_vals})[0,0,0,]\n",
    "prob = np.exp(d) / np.sum(np.exp(d))\n",
    "preds = (np.argsort(prob)[::-1])[0:5]\n",
    "for p in preds:\n",
    "    print p, class_names[p], prob[p]\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "We now start from scretch and add a fully connected network with 10 classes after the convolutional part of the network. We also have to define a loss function for training, the network later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "images = tf.placeholder(tf.float32, [None, None, None, 3], name='images')\n",
    "inputs = tf.image.resize_images(images, (224,224))\n",
    "\n",
    "with tf.variable_scope('vgg_16', [inputs]) as sc:\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                          activation_fn=tf.nn.relu,\n",
    "                          weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                          weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "        # Here we start you own definitions\n",
    "        shape = int(np.prod(net.get_shape()[1:]))\n",
    "        net = tf.reshape(net, [-1, shape])\n",
    "        net = slim.fully_connected(net, 1000, scope='fc6') #Only 1000 \n",
    "        net = slim.dropout(net, 0.5, scope='dropout6')\n",
    "        net = slim.fully_connected(net, 10, scope='fc7', activation_fn=None, normalizer_fn=None) #Only 10\n",
    "        # This would be the original vgg16. We replace these layers with FC layers and less classes\n",
    "        #net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "        #net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "        #net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None,scope='fc8')\n",
    "        \n",
    "        # The graph is stored w/o loss, adding loss to graph\n",
    "\n",
    "Y = tf.placeholder(tf.int64, shape=None, name='Labels')\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(net, Y, name='xentropy')\n",
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "sess = tf.Session()       \n",
    "tf.train.SummaryWriter('/tmp/dumm/fine_tuned', tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to restore all of the convolutional layers, we can get them as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'vgg_16/conv1/conv1_1/weights:0',\n",
       " u'vgg_16/conv1/conv1_1/biases:0',\n",
       " u'vgg_16/conv1/conv1_2/weights:0',\n",
       " u'vgg_16/conv1/conv1_2/biases:0',\n",
       " u'vgg_16/conv2/conv2_1/weights:0',\n",
       " u'vgg_16/conv2/conv2_1/biases:0',\n",
       " u'vgg_16/conv2/conv2_2/weights:0',\n",
       " u'vgg_16/conv2/conv2_2/biases:0',\n",
       " u'vgg_16/conv3/conv3_1/weights:0',\n",
       " u'vgg_16/conv3/conv3_1/biases:0',\n",
       " u'vgg_16/conv3/conv3_2/weights:0',\n",
       " u'vgg_16/conv3/conv3_2/biases:0',\n",
       " u'vgg_16/conv3/conv3_3/weights:0',\n",
       " u'vgg_16/conv3/conv3_3/biases:0',\n",
       " u'vgg_16/conv4/conv4_1/weights:0',\n",
       " u'vgg_16/conv4/conv4_1/biases:0',\n",
       " u'vgg_16/conv4/conv4_2/weights:0',\n",
       " u'vgg_16/conv4/conv4_2/biases:0',\n",
       " u'vgg_16/conv4/conv4_3/weights:0',\n",
       " u'vgg_16/conv4/conv4_3/biases:0',\n",
       " u'vgg_16/conv5/conv5_1/weights:0',\n",
       " u'vgg_16/conv5/conv5_1/biases:0',\n",
       " u'vgg_16/conv5/conv5_2/weights:0',\n",
       " u'vgg_16/conv5/conv5_2/biases:0',\n",
       " u'vgg_16/conv5/conv5_3/weights:0',\n",
       " u'vgg_16/conv5/conv5_3/biases:0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_to_restore = slim.get_variables(scope='vgg_16/con')\n",
    "[var.name for var in vars_to_restore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "restorer = tf.train.Saver(vars_to_restore)\n",
    "#restorer.restore(sess, '/Users/oli/Dropbox/server_sync/tf_slim_models/vgg_16.ckpt')\n",
    "restorer.restore(sess, '/home/dueo/Dropbox/Server_Sync/tf_slim_models/vgg_16.ckpt')\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if everything is fine in principle, we apply the untrained model to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.53580141, -15.98202515,   4.61846018, -18.85750008,\n",
       "        -13.94858742,  14.51944923,   6.84897995,   0.39248943,\n",
       "        -15.06960678,  16.3237114 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(net, feed_dict={images:feed_vals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning \n",
    "\n",
    "We are now adapting the network trained on ImageNet to do predictions on CIFAR10.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The CIFAR 10 data set\n",
    "\n",
    "The images can be obtained from: http://www.cs.toronto.edu/~kriz/cifar.html. The 32x32 images are of have the following classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['plane','auto','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    data = dict['data']\n",
    "    imgs = np.transpose(np.reshape(data,(-1,32,32,3), order='F'),axes=(0,2,1,3)) #order batch,x,y,color\n",
    "    y = np.asarray(dict['labels'], dtype='uint8')\n",
    "    return y, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y, imgs = unpickle('/Users/oli/Dropbox/data/CIFAR-10/cifar-10-batches-py/test_batch')\n",
    "y, imgs = unpickle('/home/dueo/Dropbox/data/CIFAR-10/cifar-10-batches-py/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 8, 8, 0, 6, 6, 1, 6, 3, 1], dtype=uint8), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10], imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "For training the TFLearn library (http://tflearn.org/) is quite comfortable. This library introduce some variables, which need to be initialized. We find them by the following trick: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable variables sets 4 detail: [u'vgg_16/fc6/weights:0', u'vgg_16/fc6/biases:0', u'vgg_16/fc7/weights:0', u'vgg_16/fc7/biases:0']\n"
     ]
    }
   ],
   "source": [
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"vgg_16/fc\") #The naming starts with fc1, fc2\n",
    "print(\"Number of trainable variables sets {} detail: {}\".format(len(train_vars), [u.name for u in train_vars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Following Variables have been uninitialized [u'Training_step:0', u'Global_Step:0', u'Optimizer/vgg_16/fc6/biases/Adam_1:0', u'val_loss:0', u'Optimizer/vgg_16/fc7/biases/Adam:0', u'Optimizer/vgg_16/fc7/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv4/conv4_1/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv5/conv5_2/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/fc6/weights/Adam_1:0', u'Optimizer/vgg_16/fc7/biases/Adam_1:0', u'Optimizer/vgg_16/conv4/conv4_2/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv3/conv3_2/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/beta2_power:0', u'Optimizer/vgg_16/fc7/weights/Adam:0', u'Optimizer/vgg_16/fc6/weights/Adam:0', u'Optimizer/xentropy_mean/moving_avg:0', u'Optimizer/vgg_16/conv3/conv3_1/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv2/conv2_1/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/Optimizer/Total_Loss/moving_avg:0', u'Optimizer/vgg_16/fc7/weights/Adam_1:0', u'Optimizer/vgg_16/fc6/biases/Adam:0', u'Optimizer/vgg_16/fc6/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv4/conv4_3/weights/Regularizer/l2_regularizer/moving_avg:0', u'is_training:0', u'Optimizer/vgg_16/conv5/conv5_1/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv3/conv3_3/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv1/conv1_2/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/beta1_power:0', u'Optimizer/vgg_16/conv1/conv1_1/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv2/conv2_2/weights/Regularizer/l2_regularizer/moving_avg:0', u'Optimizer/vgg_16/conv5/conv5_3/weights/Regularizer/l2_regularizer/moving_avg:0', u'val_acc:0']\n"
     ]
    }
   ],
   "source": [
    "temp = set(tf.all_variables()) #All we have \n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "trainop = tflearn.TrainOp(loss=loss, metric=None, optimizer=optimizer, trainable_vars=train_vars,batch_size=32)\n",
    "trainer = tflearn.Trainer(train_ops=trainop, tensorboard_verbose=0, session=sess, tensorboard_dir='/tmp/dumm/fine_tuned')\n",
    "uninititalized = set(tf.all_variables()) - temp\n",
    "print('The Following Variables have been uninitialized {}'.format([u.name for u in uninititalized]))\n",
    "sess.run(tf.initialize_variables(uninititalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.75346\u001b[0m\u001b[0m\n",
      "| Optimizer | epoch: 005 | loss: 0.75346 | val_loss: 1.33991 -- iter: 9000/9000\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.75346\u001b[0m\u001b[0m\n",
      "| Optimizer | epoch: 005 | loss: 0.75346 | val_loss: 1.33991 -- iter: 9000/9000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "trainer.fit({images: imgs, Y: y}, n_epoch=5, show_metric=False, val_feed_dicts=0.1, run_id='5Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](Fine_Tuning_TB.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
