{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a competition how to define a network with the least possible amount of code. The slim library includes a **arg_scope** which allows define defaults for aguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('vgg_16', [inputs]) as sc:\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                          activation_fn=tf.nn.relu,\n",
    "                          weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                          weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        images = tf.placeholder(tf.float32, [None, None, None, 3], name='images')\n",
    "        inputs = tf.image.resize_images(images, (224,224))\n",
    "        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "        net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "        net = slim.conv2d(net, 1000, [1, 1],\n",
    "                            activation_fn=None,\n",
    "                            normalizer_fn=None,\n",
    "                            scope='fc8')\n",
    "\n",
    "tf.train.SummaryWriter('/tmp/dumm/fine_tuning', tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ops = tf.get_default_graph().get_operations()\n",
    "# for i in ops:\n",
    "#    print i.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note we call the network studid, so that we don't get a hold for the tensors we need.\n",
    "variables_to_restore = slim.get_variables_to_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'vgg_16/conv1/conv1_1/weights:0',\n",
       " u'vgg_16/conv1/conv1_1/biases:0',\n",
       " u'vgg_16/conv1/conv1_2/weights:0',\n",
       " u'vgg_16/conv1/conv1_2/biases:0',\n",
       " u'vgg_16/conv2/conv2_1/weights:0',\n",
       " u'vgg_16/conv2/conv2_1/biases:0',\n",
       " u'vgg_16/conv2/conv2_2/weights:0',\n",
       " u'vgg_16/conv2/conv2_2/biases:0',\n",
       " u'vgg_16/conv3/conv3_1/weights:0',\n",
       " u'vgg_16/conv3/conv3_1/biases:0',\n",
       " u'vgg_16/conv3/conv3_2/weights:0',\n",
       " u'vgg_16/conv3/conv3_2/biases:0',\n",
       " u'vgg_16/conv3/conv3_3/weights:0',\n",
       " u'vgg_16/conv3/conv3_3/biases:0',\n",
       " u'vgg_16/conv4/conv4_1/weights:0',\n",
       " u'vgg_16/conv4/conv4_1/biases:0',\n",
       " u'vgg_16/conv4/conv4_2/weights:0',\n",
       " u'vgg_16/conv4/conv4_2/biases:0',\n",
       " u'vgg_16/conv4/conv4_3/weights:0',\n",
       " u'vgg_16/conv4/conv4_3/biases:0',\n",
       " u'vgg_16/conv5/conv5_1/weights:0',\n",
       " u'vgg_16/conv5/conv5_1/biases:0',\n",
       " u'vgg_16/conv5/conv5_2/weights:0',\n",
       " u'vgg_16/conv5/conv5_2/biases:0',\n",
       " u'vgg_16/conv5/conv5_3/weights:0',\n",
       " u'vgg_16/conv5/conv5_3/biases:0',\n",
       " u'vgg_16/fc6/weights:0',\n",
       " u'vgg_16/fc6/biases:0',\n",
       " u'vgg_16/fc7/weights:0',\n",
       " u'vgg_16/fc7/biases:0',\n",
       " u'vgg_16/fc8/weights:0',\n",
       " u'vgg_16/fc8/biases:0']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[variables_to_restore[i].name for i in range(len(variables_to_restore))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "init_assign_op, init_feed_dict = \\\n",
    "    slim.assign_from_checkpoint('/Users/oli/Dropbox/server_sync/tf_slim_models/vgg_16.ckpt', variables_to_restore)\n",
    "sess = tf.Session()\n",
    "sess.run(init_assign_op, init_feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Let's determine the race of that dog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(3)]),\n",
       " TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(1000)]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch = tf.Graph.get_tensor_by_name(tf.get_default_graph(), 'vgg_16/fc8/BiasAdd:0')\n",
    "feed.get_shape(), fetch.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 390, 460, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = imread('poodle.jpg')\n",
    "feed_vals = [img1]\n",
    "np.shape(feed_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = sess.run(fetch, feed_dict={images:feed_vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 standard poodle 0.991464\n",
      "265 toy poodle 0.0036847\n",
      "266 miniature poodle 0.00303289\n",
      "160 Afghan hound, Afghan 0.000330886\n",
      "221 Irish water spaniel 0.000312829\n",
      "355 llama 0.000262452\n",
      "244 Tibetan mastiff 0.000241841\n",
      "368 gibbon, Hylobates lar 0.000104725\n",
      "903 wig 9.71746e-05\n",
      "260 chow, chow chow 4.49079e-05\n"
     ]
    }
   ],
   "source": [
    "d = res[0,0,0,]\n",
    "prob = np.exp(d)/np.sum(np.exp(d))\n",
    "preds = (np.argsort(prob)[::-1])[0:10]\n",
    "for p in preds:\n",
    "    print p, class_names[p], prob[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting variables\n",
    "\n",
    "Sometimes it is also interesting to investigate the variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"vgg_16/conv1/conv1_1\", reuse=True):\n",
    "    var = tf.get_variable(\"weights\")\n",
    "    conv1_1 = sess.run(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b60d590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAEACAYAAACtefPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b594950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conv1_1[0,:,:,0], interpolation='none', cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
