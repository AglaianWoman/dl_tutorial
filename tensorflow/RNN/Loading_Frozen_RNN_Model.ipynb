{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a pretrained RNN charakter model. \n",
    "\n",
    "This example show a pre-trained RNN making predictions. \n",
    "\n",
    "#### Data and preprocessing\n",
    "The text for training has been extracted for the Phd thesis:\n",
    "\n",
    "Joller-Graf, Klaus, Herrn Prof Dr Wilfried Schley, and Frau Prof Dr Ingeborg Kriwet. \"Didaktik des integrativen Unterrichts.\"\n",
    "\n",
    "Which can be downloaded from: http://edudoc.ch/record/3408/files/zu05056.pdf\n",
    "\n",
    "Creating the text-file from the pdf via:\n",
    "```\n",
    "    ~/Downloads/xpdfbin-mac-3.04/bin64/pdftotext  -enc UTF-8 zu05056.pdf\n",
    "```\n",
    "http://stackoverflow.com/questions/4039930/how-to-save-text-file-in-utf-8-format-using-pdftotext\n",
    "\n",
    "This text has been preprocessed with \n",
    "```\n",
    "X, Y, char_idx = \\\n",
    "    textfile_to_semi_redundant_sequences(path, seq_maxlen=maxlen, redun_step=3)\n",
    "```\n",
    "yielding\n",
    "```\n",
    "Text total length: 1418267\n",
    "Distinct chars: 109\n",
    "Total sequences: 472748\n",
    "```\n",
    "\n",
    "### The network\n",
    "Here we use the trained network, which we obtained using the library tflearn, as follows:\n",
    "\n",
    "#### Definition\n",
    "The network has been defined in tflearn as:\n",
    "```\n",
    "g = tflearn.input_data([None, maxlen, len(char_idx)])\n",
    "g = tflearn.lstm(g, 512, return_seq=True)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.lstm(g, 512, return_seq=True)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.lstm(g, 512)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.fully_connected(g, len(char_idx), activation='softmax')\n",
    "g = tflearn.regression(g, optimizer='ADAM', loss='categorical_crossentropy',\n",
    "                       learning_rate=0.001)\n",
    "\n",
    "m = tflearn.SequenceGenerator(g, dictionary=char_idx,\n",
    "                              seq_maxlen=maxlen,\n",
    "                              clip_gradients=5.0,\n",
    "                              checkpoint_path='model_shakespeare')\n",
    "```\n",
    "Note that the network guesses the next letter using all, the hidden states of all neurons of the last layer. \n",
    "\n",
    "#### Training\n",
    "...and fitted (50 epochs) using:\n",
    "```\n",
    "for i in range(50):\n",
    "        seed = random_sequence_from_textfile(path, maxlen)\n",
    "        m.fit(X, Y, validation_set=0.1, batch_size=128,\n",
    "              n_epoch=1, run_id='shakespeare')\n",
    "        print(\"-- TESTING...\")\n",
    "        print(\"-- Test with temperature of 1.0 --\")\n",
    "        print(m.generate(600, temperature=1.0, seq_seed=seed))\n",
    "        print(\"-- Test with temperature of 0.5 --\")\n",
    "        print(m.generate(600, temperature=0.5, seq_seed=seed))    \n",
    "```\n",
    "\n",
    "#### Freezing\n",
    "finally the model has been frozen with:\n",
    "```\n",
    "# Loading the pretrained model from the checkpoint\n",
    "m.load('/home/dueo/Dropbox/__ZHAW/Projekte/RNN/model_shakespeare-166250')\n",
    "sess = m.session\n",
    "graph = tf.get_default_graph()\n",
    "input_graph_def = graph.as_graph_def()\n",
    "from tensorflow.python.framework import graph_util\n",
    "# The output node names are used to determine which \n",
    "# part of the graph needs to be frozen.\n",
    "output_node_names = \"FullyConnected/Softmax\"\n",
    "output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, # The session is used to retrieve the weights\n",
    "        input_graph_def, # The graph_def is used to retrieve the nodes\n",
    "        output_node_names.split(\",\") \n",
    "    )\n",
    "\n",
    "with tf.gfile.GFile('didactic_25.pb', \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 25 #The maximal lenth of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['l', 'p', 't', 'x', '\\x80', '\\x84', '\\x90', '\\x98', '\\x9c', '\\xa0'],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('zu05056_char_idx.pkl', 'rb') as f:\n",
    "    char_idx = pickle.load(f)\n",
    "char_idx.keys()[20:30],char_idx.values()[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 oli  staff    21M Jan  9 11:01 didactic_25.pb\r\n"
     ]
    }
   ],
   "source": [
    "# Downloading the model, if it does not exist\n",
    "import urllib\n",
    "import os\n",
    "if not os.path.isfile('didactic_25.pb'):\n",
    "    urllib.urlretrieve(\"https://dl.dropboxusercontent.com/u/9154523/models/rnn_fun/didactic_25.pb\", \"didactic_25.pb\")\n",
    "%ls -hl didactic_25.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.gfile.GFile('didactic_25.pb', \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph().as_default() \n",
    "tf.import_graph_def(graph_def,  name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ops = tf.get_default_graph().get_operations()\n",
    "#for i in ops:print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "feed = graph.get_tensor_by_name('InputData/X:0')\n",
    "fetch = graph.get_tensor_by_name('FullyConnected/Softmax:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Die Grundlagen war dabei ', 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The seed for prediction. Note that the seed need to be exactely of length maxlen\n",
    "seed = 'Die Grundlagen war dabei '[0:maxlen]\n",
    "seed, len(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a one-hot-encoded matrix\n",
    "X = np.zeros((1, maxlen, len(char_idx))) #One Batch, t, X_t (one-got-encoded)\n",
    "for t, char in enumerate(seed):\n",
    "    X[0, t, char_idx[char]] = 1.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    pred = sess.run(fetch, feed_dict={feed:X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = np.argmax(pred) #next letter\n",
    "nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code taken from from tflearn\n",
    "def reverse_dictionary(char_idx):\n",
    "    # Build reverse dict\n",
    "    rev_dic = {}\n",
    "    for key in char_idx:\n",
    "        rev_dic[char_idx[key]] = key\n",
    "    return rev_dic\n",
    "\n",
    "rev_dic = reverse_dictionary(char_idx)\n",
    "rev_dic[nl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.28207513e-31   8.79923726e-27   2.53016997e-17   6.91295066e-09\n",
      "   1.32553896e-06   1.37966583e-13   9.54707668e-19   4.14432500e-18\n",
      "   2.87888716e-22   9.76138693e-23   1.25063598e-05   3.12398640e-07\n",
      "   8.62108391e-06   5.36712787e-06   3.30259354e-07   9.97228895e-08\n",
      "   6.30583538e-17   2.23483215e-20   6.56186163e-01   9.55695577e-04\n",
      "   1.04015914e-03   1.72303675e-03   2.04624259e-04   5.79821146e-22\n",
      "   1.27256571e-12   1.69648229e-30   1.63668226e-24   7.39453973e-19\n",
      "   4.12774693e-16   4.11367606e-17   2.95026070e-12   1.58994764e-19\n",
      "   1.35773126e-18   9.81848310e-24   1.12997094e-16   9.87313885e-15\n",
      "   6.21285009e-27   6.17528923e-13   2.80104386e-28   1.02876789e-12\n",
      "   6.40105441e-07   1.16727833e-05   5.11848270e-08   1.21353034e-04\n",
      "   3.15928759e-08   8.67063257e-13   1.02493604e-25   4.58895926e-07\n",
      "   1.11403149e-02   2.59886961e-03   2.01770250e-04   3.95688638e-02\n",
      "   3.08613107e-03   1.54157338e-31   2.04423276e-16   2.76579860e-24\n",
      "   3.06373736e-17   4.78315965e-23   3.08699231e-15   1.25488433e-22\n",
      "   5.16755604e-28   3.04888630e-08   2.37077740e-12   6.41219606e-19\n",
      "   8.97492303e-09   3.30127479e-31   4.67932341e-06   1.42025385e-06\n",
      "   2.21803877e-11   1.51903009e-07   6.59720172e-07   1.76591346e-08\n",
      "   5.67806069e-07   1.68473009e-04   3.44702159e-04   9.58797609e-05\n",
      "   1.35227591e-01   2.56636005e-04   2.00980948e-03   1.50054647e-03\n",
      "   4.09283933e-26   1.57931834e-22   1.45101687e-23   5.07815803e-26\n",
      "   1.46209870e-27   3.98835797e-25   3.94349386e-10   4.44751701e-11\n",
      "   1.21606081e-13   1.23774749e-11   1.40182919e-19   3.03802553e-19\n",
      "   1.21811149e-06   2.49370565e-28   1.97510940e-06   8.47964384e-06\n",
      "   5.88892476e-07   6.94807580e-12   4.64003597e-06   3.81357122e-22\n",
      "   1.84210422e-30   1.18488213e-03   6.19942512e-05   4.34766337e-02\n",
      "   8.88138711e-02   9.07009840e-03   8.38682812e-09   8.96042387e-04\n",
      "   1.49458047e-27]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18, 'd')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    logit = np.log(a) \n",
    "    p = np.exp(logit / temperature)\n",
    "    #1.001 to be on the save side, sum(p) < 1 for np.random.multinomial\n",
    "    p = p / (1.001 * np.sum(p))\n",
    "    return np.argmax(np.random.multinomial(1, p, 1))\n",
    "\n",
    "n = _sample(pred[0])\n",
    "n, rev_dic[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den Me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thodischen Unterricht ein eigene Vorstellung von Handlungen bereits von einer wissenschaftlichen Umfang, wie muss Brichten dienen muss: ���Das Gelaufende k��nnen mit Sichten, die einzelnen Teil so etwas genommen wurden. Das ist mit dem aufgef��hrten T��tigkeit ein emotioniertes Unterrichts bezeichnet, so steht eine F��hle, die genau in unterst��tzenden Struktur bestimmte Modelle sind von den\n",
      "\n",
      "Die Grundlagen war dabei den Methodischen Unterricht ein eigene Vorstellung von Handlungen bereits von einer wissenschaftlichen Umfang, wie muss Brichten dienen muss: „Das Gelaufende können mit Sichten, die einzelnen Teil so etwas genommen wurden. Das ist mit dem aufgeführten Tätigkeit ein emotioniertes Unterrichts bezeichnet, so steht eine Fühle, die genau in unterstützenden Struktur bestimmte Modelle sind von den\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Code adapted from tflearn SequenceGenerator\n",
    "def generate(sess, seq_seed, show=True, seq_length = 400, temperature = 0.1,  seq_maxlen=25):\n",
    "    sequence = seq_seed\n",
    "    generated = seq_seed\n",
    "    dic = char_idx\n",
    "    rev_dic = reverse_dictionary(dic)\n",
    "\n",
    "\n",
    "    whole_sequence = seq_seed\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        X = np.zeros((1, seq_maxlen, len(dic)))\n",
    "        for t, char in enumerate(sequence):\n",
    "            X[0, t, dic[char]] = 1.\n",
    "        preds = sess.run(fetch, feed_dict={feed:X})[0] #Getting next letter distribution\n",
    "        next_index = _sample(preds, temperature) #Sampling a letter from the distribution\n",
    "        #next_index = np.argmax(preds)\n",
    "        next_char = rev_dic[next_index]\n",
    "        if show:\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        generated += next_char\n",
    "        sequence = sequence[1:] + next_char\n",
    "        whole_sequence += next_char\n",
    "    return whole_sequence\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res = generate(sess, seed, temperature=1.0)\n",
    "    print('\\n')\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some further examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Grundlagen war dabei nach abschliessen. „In der eigene Prozesses Lerngebundents in den Interviews jegentliche Planungsprobleme dadurch, dass die Kinder lässt sich Akzeptanz. Das ist erhöht den Bedeutung nötige Delitivität bringen, die Stofferfachung oder dem Kerngegenständen zur Forschungs- und Zeit“ (Hanna Z./MS2).\n",
      "319\n",
      "\n",
      "\f",
      "Didaktik des integrativen Unterrichts\n",
      "Überlegungen für Integration in die Rolle von As\n",
      "\n",
      "Temperature 1.0\n",
      "Die Grundlagen war dabei die Weiterbildung ein zu verhalten, bzw. bei Kinder und Jugendliche neaber lernbehinderter und Lehr- und Lehren (‚Abers für Analyse: Eines der gesellschaftlichenden sich ableiten zu führen. „Wir schlechten Experten sollte nicht mehr anrizerieren. Sie müssen die Monoten begegnet sich es „geübt (vgl. Abb. 4.7) aus verschiedenen Problemen. Wir hätten wir mit aufgesetzt werden nie für den \n",
      "\n",
      "Temperature 0.5\n",
      "Die Grundlagen war dabei die Aussage, dass die Verantwortung und gewissermassen die Lehrperson die Lehrpersonen sehr unterschiedlichen Bedeutung und Fähigkeit und Gesprächen der Gruppen auszugehen.\n",
      "„Die Schülerinnen und Schüler auf den Prozess eine Schülerinnen und Schüler mit den eigenen Lehrpersonen ein gewisser Aspekt in der Praxis ist es in der Schulische Heilpädagoge sind und zwei Lehrpersonen die Wirkung in\n",
      "\n",
      "Temperature 0.5\n",
      "Die Grundlagen war dabei die Schule sicher widersprüchlich zu den Motivation erwähnt wird, das einerseits steht es eine Position eine Schule und Aussagen auf den Unterricht bestehen wird. Die grosse Schwierigkeit steht, das entsprechend sind die Ziele gesehen werden, die mit dem Lehren und Regeln in der Probleme in der Lehrperson immer wieder die diese Arbeit von Lehrpersonen und sie ergeben sich die die Bestehenspeziel\n",
      "\n",
      "Temperature 0.1\n",
      "Die Grundlagen war dabei die Schülerinnen und Schüler sehr gut gesehen. Das ist auch die Lehrperson die Lehrpersonen die Lehrpersonen die Lehrperson die Lehrpersonen die Schülerinnen und Schüler sehr gut auf der anderen Seite die Lehrpersonen die Lehrpersonen die Lehrperson die Lehrpersonen die Schülerinnen und Schüler in der Schule der Unterstützung der Lehrperson die Lehrpersonen die Lehrpersonen die Lehrpersonen\n",
      "\n",
      "Temperature 0.05\n",
      "Die Grundlagen war dabei die Schülerinnen und Schüler sehr gut gesehen. Das ist ein gesellschaftlicher und die Schulische Heilpädagogin oder der Heilpädagoge auf dem Struktur integriert werden kann, dass die Lehrperson die Schülerinnen und Schüler sehr gut gesehen. Das ist ein gesellschaftliches Anspruch von Schülerinnen und Schüler sehr gut gesehen. Das ist auch die Schülerinnen und Schüler sehr gut gesehen. Da\n"
     ]
    }
   ],
   "source": [
    "ts = (1.0, 1.0, 0.5, 0.5, 0.1, 0.05)\n",
    "with tf.Session() as sess:\n",
    "    for t in ts:\n",
    "        print()\n",
    "        print(\"Temperature {}\".format(t))\n",
    "        print(generate(sess, seed, temperature=t, show=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some observations:\n",
    "\n",
    "* For low temperatures, the system gets trapped in a kind of local minima.\n",
    "* At least it's political correct:'Schülerinnen und Schüler, 'Heilpädagogin oder dem Heilpädagogen'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
