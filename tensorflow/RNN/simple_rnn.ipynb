{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0-rc1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "\n",
    "In this notebook we consider a simple example of an RNN and used a quite artifical data generating process. The example has been adopted from:\n",
    "http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html. \n",
    "\n",
    "Other Resources for RNNs:\n",
    "\n",
    "* http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "* http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the task\n",
    "\n",
    "We consider a network which predicts at each point in time a variable $\\hat{y}_t$. (It thus corresponds to the \"many to many\" example at the very right side of the figure from the famous Karpathy blog post http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example data  (I screama, you screama, we all screama for I screama)\n",
    "\n",
    "We need some data to play around with RNNs. They are capable of doing quite complicated things such as language models and so on. For this example, we want to generate the data ourself. We have to come up with a process which creates $x_t$ which itself can be influcenced by events $x_{t'}$ which happend before $t$. Further, we have to come up with $y_t$ which depends on $x_t'$ for timepoints $t' \\le t$. \n",
    "\n",
    "To keep it simple, we analyse the following quite artifical process in which the weather $x_{t'}$ for $t' \\le t$ influences our stock on icecream $y_t$. We then see if the RNN is capable of reconstructing that process.\n",
    "\n",
    "#### Definition of the simple process\n",
    "The weather $x_t$ at a certain point in time $t$ has three states (sunny, rainy, cloudy), which we model as $x_t = (1,0,0)$, $x_t = (0,1,0)$, and $x_t = (0,0,1)$ repectively. We assume that the weather is completly random (of course we could model more complex scenarios). \n",
    "\n",
    "We have an icecream store capable of holding 2 units of icecream and we start with a full store. When it is sunny we sell one unit of icecream. We have the strange policy that we order  on unit of icecream when it's claudy. It takes 3 days to deliever the ice cream, we accept the ice cream if we do not have a full stock.\n",
    "\n",
    "This enables us to model $y_t$ the state of the store $(1,0)$ for out of stock and $(0,1)$ for in stock. We create the one-hot-encoded data in the graph later. For now we use integers but keep in mind that the data is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    Xs = np.array(np.random.choice(3, size=(size,))) #Random Weather\n",
    "    Y = []\n",
    "    ice = 2 #Our stock of icecream at start\n",
    "    for t,x in enumerate(Xs):\n",
    "        # (t-3) >= 0 the first ice cream could be delivered on day 3\n",
    "        # Xs[t - 3] claudy three days before today => we ordered ice cream\n",
    "        # ice < 2 not full\n",
    "        if (t - 3) >= 0 and Xs[t - 3] == 1 and ice < 2: \n",
    "            ice += 1\n",
    "        if x == 0: # It is sunny we therefore sell ice, if we have\n",
    "            if ice > 0: # We have ice cream\n",
    "                ice -= 1\n",
    "        if ice > 0: #We are not out of stock\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "    return Xs, np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 2, 0, 2, 1, 1, 2, 2, 1, 0, 0, 2, 2, 2, 0, 1, 0, 0, 2, 0, 2, 2,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0,\n",
       "        1, 2, 1, 2]),\n",
       " array([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = gen_data(50000) #Global variables holding the input and output\n",
    "(X_train[0:50], Y_train[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the Minibatch\n",
    "\n",
    "In this example, we have in principle a large stream of data $x$ and $y$. For efficiency reason we split the stream in minibatches of a certain length. For this task we could also imagin to have several realizations of that icecream process, so that it would also be natural to split the process into mini batches. \n",
    "\n",
    "For simplicyty we create the minibatch by we randomly cutting out `batch_size` entries of fixed length `num_steps`. Other, more advanced ways of doing so are possible. See e.g. https://danijar.com/variable-sequence-lengths-in-tensorflow/. For the time being, we thus consider the input tensor $X_{btc}$ for the minibatch to be of the following form:\n",
    "\n",
    "* $b$ having `batch_size` entries\n",
    "* $t$ loops over the unrolled timestamps (`num_steps`)\n",
    "* $c$ has the dimension of the one-hot-coded classes (the one-hot-encoding will be done in the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(Xs, Ys, batch_size = 32, num_steps = 50):\n",
    "    data_x = np.zeros([batch_size, num_steps], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, num_steps], dtype=np.int32)\n",
    "    for i in range(1,batch_size):\n",
    "        s = int(np.random.uniform(0, len(Xs) - num_steps))\n",
    "        data_x[i] = Xs[s : s + num_steps]\n",
    "        data_y[i] = Ys[s : s + num_steps]  \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 1 1 1 0]\n",
      " [1 1 0 2 2 2 1 0 0 2]]\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 1 1]\n",
      " [1 1 1 1 1 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_batch(X_train, Y_train,3, 10)\n",
    "print X\n",
    "print Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 40     # number of truncated backprop steps\n",
    "batch_size = 200  # number of minibatches b\n",
    "num_classes_in = 3   # number of classes in the input\n",
    "num_classes_out = 2   # number of classes in the output\n",
    "state_size = 5    # number of classes in the state\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the in- and outputs\n",
    "\n",
    "We define the input and output for the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose:0' shape=(200, 40, 3) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# RNN Inputs\n",
    "# One hot encoding.\n",
    "x_one_hot = tf.one_hot(x, num_classes_in)\n",
    "# We want the following dimensions [batch_size, Max_Length, num_classes_in]\n",
    "rnn_inputs = tf.transpose(x_one_hot, perm=(0,1,2))\n",
    "rnn_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input tensor $r_{btc}$  is indexed by \n",
    "\n",
    "* $b$ having `batch_size` entries\n",
    "* $t$ loops over the unrolled timestamps\n",
    "* $c$ has the dimension of the one-hot-coded classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition the cell\n",
    "\n",
    "We now define the network, we do not consider the output nodes yet.\n",
    "A single RNN cell is shown in the figure below in the middle:\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "Image taken from: [Colah's RNN Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "The joining of the two lines coming from the previous state $h_{t-1}$ and the current x-values $x_t$ reflects a concantination to a vector  $[h_{t-1}, x_{t}]$ of size `state_size + num_classes_in`. Alternatively, instead of concatinating, one could also use two matrices $W_x$ and $W_h$ and keep the states seperate. This is mathematically completely identical. The new state $h_t$ is \n",
    "then calculated as:\n",
    "\n",
    "$$\n",
    "    h_{t} = \\tanh([h_{t-1}, x_{t}] \\cdot W + b) = \\tanh(h_{t-1} \\cdot W_h + x_{t} \\cdot W_x + b)\n",
    "$$\n",
    "\n",
    "Note that we share the variables W and b during different time-points. To do this in TensorFlow we define them in a first step for later reuse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of the Variables needed in a single cell\n",
    "with tf.variable_scope('rnn_cell', reuse = False):\n",
    "    W = tf.get_variable('W', [num_classes_in + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "# Definition of a single cell\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes_in + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat(1, [state, rnn_input]), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unrolling the timesteps\n",
    "\n",
    "We build the network using the identical weights in the `num_steps` unrolled timesteps. Techniqualy the `rnn_cell` at time t, gets the input (weather) at time t and the pervious state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pack:0' shape=(200, 40, 5) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_state\n",
    "rnn_outputs_l = []\n",
    "\n",
    "for t in range(num_steps):\n",
    "    state = rnn_cell(rnn_inputs[:,t,:], state) #Pervious state\n",
    "    rnn_outputs_l.append(state) #We put the states h_0, h_1, ... in a list. \n",
    "\n",
    "rnn_outputs = tf.pack(rnn_outputs_l, axis=1)\n",
    "rnn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding output to the network\n",
    "\n",
    "We now add at each timepoint an output layer to the network. This output use the same weiht for each timepoint.\n",
    "\n",
    "The output of the rnn is a tensor index by $o_{btk}$ (batch, time, class). This tensor produces for each minibatch and timepoint a number of output states dimensional vector indexed by $k$. This output needs to be compared with the y-value with has the shape $y_{bt}$. It's easier later on to flatten this vector for each batch.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Reshape_1:0' shape=(8000,) dtype=int32>,\n",
       " <tf.Tensor 'Reshape:0' shape=(8000, 5) dtype=float32>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "y_reshaped, rnn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    V = tf.get_variable('V', [state_size, num_classes_out])\n",
    "    b = tf.get_variable('b', [num_classes_out], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, V) + b\n",
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looks quite ugly\n",
    "# writer = tf.train.SummaryWriter(\"/tmp/dumm/RNN2\", tf.get_default_graph(), 'graph.pbtxt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-f0b66b37e744>:2 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 0.691968500614\n",
      "1 0.640196859837\n",
      "2 0.624058187008\n",
      "3 0.585162341595\n",
      "4 0.568682193756\n",
      "5 0.502234458923\n",
      "6 0.460208594799\n",
      "7 0.432763636112\n",
      "8 0.34563729167\n",
      "9 0.382720649242\n",
      "200 0.170340090288\n",
      "400 0.147368130423\n",
      "600 0.147375527583\n",
      "800 0.148459675126\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "count = 0\n",
    "sum_tr_losses = 0\n",
    "for i in range(1000):\n",
    "    X, Y = get_batch(X_train, Y_train, batch_size, num_steps)\n",
    "    tr_losses, _ = \\\n",
    "    sess.run([total_loss, train_step], feed_dict={x:X, y:Y})\n",
    "    sum_tr_losses += tr_losses\n",
    "    count += 1\n",
    "    if (i < 10) or (i % 200 == 0):\n",
    "        print \"{} {}\".format(i, sum_tr_losses / count)\n",
    "        count = 0\n",
    "        sum_tr_losses = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(logits)\n",
    "X, Y = get_batch(X_train, Y_train, batch_size, num_steps)\n",
    "p_y_pred = sess.run(preds, feed_dict={x:X}) #A list for each time point\n",
    "loss_train = sess.run(total_loss, feed_dict={x:X, y:Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1576525"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95825000000000005"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(p_y_pred, axis=1) == np.reshape(Y, -1)) / (1.0 * batch_size * num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Change the state size to 2. What do you obsereve? Give an explanation for your observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using the TensorFlow API\n",
    "\n",
    "Alternatively one can use the TensorFlow-API for creating RNNs. In principle there are two TensorFlow methods. The first, kind of deprecated one, builds a graph from the unrolled network. This API has issues in performance, first of all the creation of the graph takes quite some time. Further, and this is a bit it is also slower during runtime. Therefore, the novel dynamic API should be prefered. If you want to use sequences of variable length see:  https://danijar.com/variable-sequence-lengths-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose:0' shape=(200, 40, 3) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# RNN Inputs\n",
    "# One hot encoding.\n",
    "x_one_hot = tf.one_hot(x, num_classes_in)\n",
    "# We want the following dimensions [batch_size, Max_Length, num_classes_in]\n",
    "rnn_inputs = tf.transpose(x_one_hot, perm=(0,1,2))\n",
    "rnn_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN/transpose:0' shape=(200, 40, 5) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output $o_{btk}$ tensor produces for each minibatch and timepoint a 4 (number of output states) dimensional vector indexed by $k$. This can be compared with the y-value with has the shape $y_{bt}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Reshape_1:0' shape=(8000,) dtype=int32>,\n",
       " <tf.Tensor 'Reshape:0' shape=(8000, 5) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "y_reshaped = tf.reshape(y, [-1])\n",
    "y_reshaped, rnn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes_out])\n",
    "    b = tf.get_variable('b', [num_classes_out], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "logits = tf.matmul(rnn_outputs, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-e16a93bed860>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 35.462366432\n",
      "1 0.691848635674\n",
      "2 0.682622790337\n",
      "3 0.670633971691\n",
      "4 0.662271261215\n",
      "5 0.658096909523\n",
      "6 0.653116941452\n",
      "7 0.635891377926\n",
      "8 0.618283689022\n",
      "9 0.592325210571\n",
      "200 0.178685860601\n",
      "400 0.0938078673556\n",
      "600 0.0905056070909\n",
      "800 0.0891324103251\n"
     ]
    }
   ],
   "source": [
    "Y = None\n",
    "X = None\n",
    "count = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    training_losses = []\n",
    "    for i in range(1000):\n",
    "        X, Y = get_batch(X_train, Y_train, batch_size, num_steps)\n",
    "        tr_losses, _ = sess.run([total_loss, train_step], feed_dict={x:X, y:Y})\n",
    "        count += 1\n",
    "        sum_tr_losses += tr_losses\n",
    "        if (i < 10) or (i % 200 == 0):\n",
    "            print \"{} {}\".format(i, sum_tr_losses / count)\n",
    "            count = 0\n",
    "            sum_tr_losses = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
