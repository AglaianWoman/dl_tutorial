{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping \n",
    "\n",
    "### What for\n",
    "\n",
    "First, the **layout in tensorboard** respects scopes and thus creates nice structured layouts in which operations and weight with names like 'conv1/bias' and 'conv1/weights' are grouped together. If you whish, you can create these names by hand. However the scoping mechanism has additional benefit as we see below.\n",
    "\n",
    "Another use case for name scoping is to make **building blocks** which can be used to assemble a network. There are two mechanims around. Variable and name scopes, I recommend using variable scopes which you can also use to share weights (see below). For the difference see http://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'conv1/Kernels:0', u'conv1/Conv2D:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('conv1/'):\n",
    "    net = tf.placeholder(dtype='float32', \n",
    "                         shape=(None, 64, 64, 3), name='Input')\n",
    "    W = tf.get_variable('W', shape=(3,3,3,10)) \n",
    "    var = tf.Variable(tf.truncated_normal((3,3,3,10)), \n",
    "                      name='Kernels')\n",
    "    ops = tf.nn.conv2d(net, W, (1,1,1,1), padding='SAME')\n",
    "var.name, ops.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using name scoping as a building block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def conv_layer(net, shape, scope):\n",
    "    with tf.variable_scope(scope) as v_scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal(shape, dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(net, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[shape[3]], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        return tf.nn.relu(out, name=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = tf.placeholder(dtype='float32', shape=(None, 64, 64, 3), name='Input')\n",
    "net = conv_layer(net, [3, 3, 3, 64], 'conv1')\n",
    "net = conv_layer(net, [3, 3, 64, 128], 'conv2')\n",
    "net = conv_layer(net, [3, 3, 128, 128], 'conv3')\n",
    "writer = tf.train.SummaryWriter(\"/tmp/dumm/scoping2\", tf.get_default_graph(), 'graph.pbtxt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tb_scoping.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Scope\n",
    "Variable scoping is a mechanism to share the variables of (possible large) parts of a network. These shared variables are very useful e.g. for Siamese Networks. See also https://www.tensorflow.org/versions/master/how_tos/variable_scope/index.html\n",
    "\n",
    "#### The two functions to use together\n",
    "* `tf.variable_scope` created the name-space or better context manager\n",
    "* `tf.get_variable()` gets or newly creates variables in the name scope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'foo/bar/v:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('foo'):\n",
    "    with tf.variable_scope('bar'):\n",
    "     d = tf.get_variable('v', shape=(1,10))\n",
    "d.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reusing variables vs new variables\n",
    "\n",
    "##### Creating new variables\n",
    "The context manager is creating new variables by default. If you request a variable, which has already been created it returns an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'var/a:0', u'var/a2:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('var'):\n",
    "    a1 = tf.get_variable('a', shape=(1))\n",
    "    #This variable is used and thus this would result in an error\n",
    "    #a1_1 = tf.get_variable('a', shape=(1)) \n",
    "    a2 = tf.get_variable('a2', shape=(1))    \n",
    "a1.name,a2.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reusing variables\n",
    "\n",
    "Reusing variables is important when it come to share variables such as in Siamese networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'var/a:0', u'var/a:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('var', reuse=False):\n",
    "     a1 = tf.get_variable('a', shape=(1))\n",
    "\n",
    "with tf.variable_scope('var', reuse=True):\n",
    "    a1_1 = tf.get_variable('a', shape=(1)) #This variable is reused\n",
    "    #This would give an error, since that variable has not been used before\n",
    "    #a2 = tf.get_variable('a2', shape=(1)) \n",
    "a1.name, a1_1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'var/a:0', u'var/a:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('var', reuse=False) as var_context:\n",
    "    a1 = tf.get_variable('a', shape=(1))\n",
    "    var_context.reuse_variables()\n",
    "    #Alternatively tf.get_variable_scope().reuse_variables()\n",
    "    a1_1 = tf.get_variable('a', shape=(1))\n",
    "a1.name,a1_1.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
